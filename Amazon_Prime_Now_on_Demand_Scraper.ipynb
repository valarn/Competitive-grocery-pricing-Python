{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install selenium\n",
    "import pandas as pd, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from time import sleep\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: The function takes in a zipcode, list of categories and the desired filename\n",
    "# Returns a csv file with all of the products in the categories with all of the\n",
    "# information associated with them.\n",
    "# The scraper will go thru Wholefoods, Bristol Farms and the Amazon online grocery listings\n",
    "\n",
    "def amazon_beast(zip_code, cats, filename):\n",
    "    driver = webdriver.Chrome(executable_path=\"./Chromedriver/macos/chromedriver\") \n",
    "    # Enter the website and push in the desired zipcode\n",
    "    url = 'https://primenow.amazon.com/browse?node=17133246011&ref_=pn_br_ct_17133246011_1&pf_rd_r=YNH3ZVKAJVWTKGRCG28C&pf_rd_p=630aba01-d0d4-414a-9ab5-fe6697775232&pf_rd_s=desktop-center-1&pf_rd_i=17170716011&pf_rd_t=101&pf_rd_m=A35T2T8BW467U4'\n",
    "    driver.get(url)\n",
    "    zipcode = driver.find_element_by_id('lsPostalCode')\n",
    "    zipcode.send_keys(zip_code)\n",
    "    submit_zipcode = driver.find_element_by_class_name('a-button-input')\n",
    "    submit_zipcode.click()\n",
    "    \n",
    "    \n",
    "    #Finding the categories of the zip_code\n",
    "    sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    categories = soup.find_all(\"li\",attrs = {\"class\": lambda e: e.startswith(\"filter__subCategory\") if e else False})\n",
    "    category_list=[]\n",
    "    for cat in categories:\n",
    "        category={}\n",
    "        category['category'] = cat.get_text()[:-1]\n",
    "        category['link']=(cat.find({\"a\":\"href\"})['href'])\n",
    "        category_list.append(category)\n",
    "\n",
    "    categoryMeat = {'category':'Meat & Seafood','link':\"/browse?ref=lp_null_dri_14_17395961011&ie=UTF8&node=17395961011\"}\n",
    "    categoryBread =  {'category':'Bread & Bakery','link':\"/browse?ref=lp_null_dri_3_17127585011&ie=UTF8&node=17127585011\"}\n",
    "    category_list.append(categoryMeat)\n",
    "    category_list.append(categoryBread)\n",
    "\n",
    "    \n",
    "    # Standardizing the categories \n",
    "    for cat in category_list:\n",
    "        if cat['category']== 'All':\n",
    "            category_list.remove(cat)\n",
    "\n",
    "    Beverages = ['Water','Sparkling Water','Enhanced Water','Soda','Juices']\n",
    "    Dairy_Cheese_Eggs = ['Milk & Cream', 'Cheese', 'Yogurt','Eggs & Egg Substitutes',\n",
    "    'Butter & Margarine','Milk Substitute']\n",
    "    Produce = ['Fresh Fruits','Fresh Vegetables']\n",
    "    Frozen_Foods = ['Meals & Entrees','Pizza']\n",
    "    Pantry = ['Canned, Jarred & Packaged Food','Pasta & Noodles',\n",
    "              'Condiments & Salad Dressings','Cooking & Baking','Jams & Peanut Butte']\n",
    "    Snacks_Sweets = ['Chips & Crisps','Crackers','Popcorn','Pretzels','Salsas, Dips, & Spread'\n",
    "    ,'Ice Cream','Cookies','Candy & Chocolate','Puddings & Gelatin',\n",
    "    'Granola & Nutrition Bars','Rice Cakes','Nuts & Seed', 'Nuts, Seeds, & Snack Mi','Dried Fruits & Vegetables']\n",
    "    Breakfast_Cereals = ['Cereals','Breakfast & Cereal Bars','Pancake, Waffle, & Baking Mixes',\n",
    "                        'Breakfast Syrups & Topping', 'Breakfast Food']\n",
    "    Coffee_Tea = ['Coffee','Tea']\n",
    "    Energy_drinks = ['Sports & Energy Drink']\n",
    "\n",
    "    for i in category_list:\n",
    "        if i['category'] in Beverages:\n",
    "            i['category'] = 'Beverages'\n",
    "        if i['category'] in Dairy_Cheese_Eggs:\n",
    "            i['category'] = 'Dairy'\n",
    "        if i['category'] in Produce:\n",
    "            i['category'] = 'Produce'\n",
    "        if i['category'] in Frozen_Foods:\n",
    "            i['category'] = 'Frozen Foods'\n",
    "        if i['category'] in Pantry :\n",
    "            i['category'] = 'Pantry'\n",
    "        if i['category'] in Snacks_Sweets:\n",
    "            i['category'] = 'Snacks & Sweets'\n",
    "        if i['category'] in Breakfast_Cereals:\n",
    "            i['category'] = 'Breakfast & Cereals'\n",
    "        if i['category'] in Coffee_Tea:\n",
    "            i['category'] = 'Coffee & Tea'\n",
    "        if i['category'] in Energy_drinks:\n",
    "            i['category'] = 'Sports & Energy Drinks'\n",
    "      \n",
    "    category_list = pd.DataFrame(category_list) \n",
    "    for index, row in category_list[category_list['category'].isin(cats)].iterrows():\n",
    "        driver.get('https://primenow.amazon.com'+row['link'])\n",
    "        print(f\"Scraping category: {row['category']}\")\n",
    "\n",
    "        try:\n",
    "              number_of_pages = int(driver.find_elements_by_xpath('//span[@class = \"buttons__paginate-link-text__30gGN\"]')[-1].text)\n",
    "        except:\n",
    "              number_of_pages = 1\n",
    "        print(f'{number_of_pages} pages are getting scraped')\n",
    "        product_link_list = []\n",
    "        for count in range(number_of_pages):\n",
    "            if number_of_pages != 1:\n",
    "                if count==0:\n",
    "                    next_page = driver.find_elements_by_xpath('//li[@class = \"buttons__pagination-item__1mFvI\"]')[-1]\n",
    "                    next_page_url = next_page.find_element_by_css_selector('a').get_attribute('href')\n",
    "                else:\n",
    "                    driver.get(next_page_url)\n",
    "\n",
    "\n",
    "\n",
    "            sleep(1)\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    #       products = soup.find_all('li',attrs={'class': lambda e: e.startswith('product_grid__item') if e else False})\n",
    "            links = soup.find_all('div', attrs ={'class': lambda e: e.startswith('asin_offer__root') if e else False})\n",
    "            print(f\"{len(links)} products this page\")\n",
    "    #       Finds the link category and the store for each of the links\n",
    "            for link in links:\n",
    "                product_link_category_store={}\n",
    "                product_link_category_store['product_link'] = (link.find({'a':'href'})['href'])\n",
    "                product_link_category_store['category'] = row['category']\n",
    "                product_link_category_store['store'] = str(link.find('img').get('alt'))\n",
    "                product_link_list.append(product_link_category_store)\n",
    "\n",
    "\n",
    "            # Captures the products that are offered in more than one store.\n",
    "            more_offers = driver.find_elements_by_css_selector(\"div[class^='asin_offer_popover__additionOfferHeading']\")\n",
    "            for offer in more_offers:\n",
    "                offer.click()\n",
    "                \n",
    "                soup = BeautifulSoup(driver.page_source,'lxml')         \n",
    "                store_offer = soup.find_all('div', attrs ={'class': lambda e: e.startswith('asin_offer_popover__additionalOfferRow') if e else False})\n",
    "                for offer in store_offer:\n",
    "                    product_link_category_store={}\n",
    "                    product_link_category_store['product_link'] = (offer.find({'a':'href'})['href'])\n",
    "                    product_link_category_store['category'] = category['category']\n",
    "                    logo = offer.find('div',attrs = {'class': lambda e: e.startswith('asin_offer_price__logoContainer') if e else False})\n",
    "                    product_link_category_store['store'] = str(logo.find('img').get('alt'))\n",
    "                    product_link_list.append(product_link_category_store)\n",
    "\n",
    "\n",
    "\n",
    "            next_page_url = next_page_url.replace(f'page={count+1}',f'page={count+2}')\n",
    "            next_page_url = next_page_url.replace(f'_pg_{count+1}', f'_pg_{count+2}')\n",
    "\n",
    "              \n",
    "              \n",
    "    print(len(product_link_list))\n",
    "    product_list=[]\n",
    "    # For the specified category it loops thru the pages\n",
    "    for product in product_link_list:\n",
    "        product_info = {}\n",
    "        driver.get('https://primenow.amazon.com'+product['product_link'])\n",
    "        soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        #Saving the  name of the product\n",
    "        name = soup.find('div', attrs = {'class': lambda e: e.startswith('a-section a-spacing-none') if e else False}).get_text()\n",
    "        product_info['name'] = name.strip()\n",
    "        print(f\"Scraping product: {product_info['name']} from the {product['category']} category\" )\n",
    "\n",
    "        #Saving the brand of the product      \n",
    "        brand = soup.find('div',{'id':'bylineInfo_feature_div'}).get_text()\n",
    "    #   m =  re.(r'[^\\bby]', brand0)\n",
    "        product_info['brand'] = brand.strip().split('\\n')[-1].strip()\n",
    "        #Standardized category of the product\n",
    "        product_info['category'] = product['category']\n",
    "        #Store that offers the product\n",
    "        product_info['store'] = product['store']\n",
    "        #Saving the price of the product\n",
    "        try:\n",
    "            try:\n",
    "                price = soup.find('span',{'id':'priceblock_ourprice'}).get_text()\n",
    "                product_info['price'] = price[1:]\n",
    "            except:\n",
    "                # if on sale this is how its find\n",
    "                price = soup.find('span', {'class':'a-text-strike'}).get_text()\n",
    "                product_info['price'] = price[1:]\n",
    "        except:\n",
    "              pass\n",
    "\n",
    "        #Saving the description of the product\n",
    "        try:\n",
    "            description = soup.find('div', {'id':'productDescription'}).get_text()\n",
    "            product_info['description'] = description.strip()\n",
    "        except:\n",
    "            product_info['description'] = 'NaN'\n",
    "\n",
    "        #Saving the information of the product\n",
    "        try:\n",
    "            information = soup.find('div',{\"id\": \"important-information\"})\n",
    "            if information is not None:\n",
    "                product_info['important_info'] = information.get_text().strip()\n",
    "        except:\n",
    "            information = soup.find('div',{\"id\": \"importantInformation\"})\n",
    "            if information is not None:\n",
    "                product_info['important_info'] = information.get_text().strip()\n",
    "\n",
    "        # Finding the UPC and the ASIN of the product from the 'Product information Table'\n",
    "            try:\n",
    "                table = soup.find('table', {\"id\":'productDetails_detailBullets_sections1'})\n",
    "                rows = table.find_all('tr')\n",
    "                for row in rows:\n",
    "                    col = row.find('th', {'class': 'a-color-secondary a-size-base prodDetSectionEntry'}).get_text().strip()\n",
    "                    if col =='UPC':     \n",
    "                        UPC = row.find('td',{'class':'a-size-base'}).get_text().strip()\n",
    "                        product['UPC'] = UPC\n",
    "                    if col =='ASIN':     \n",
    "                        ASIN = row.find('td',{'class':'a-size-base'}).get_text().strip()\n",
    "                        product['ASIN']= ASIN\n",
    "\n",
    "            except:\n",
    "                table = soup.find('table', {\"id\":'productDetails_techSpec_section_1'})\n",
    "                rows = table.find_all('tr')\n",
    "                for row in rows:\n",
    "                    col = row.find('th', {'class': 'a-color-secondary a-size-base prodDetSectionEntry'}).get_text().strip()\n",
    "                    if col =='UPC':     \n",
    "                        UPC = row.find('td',{'class':'a-size-base'}).get_text().strip()\n",
    "                        product['UPC'] = UPC\n",
    "                    if col =='ASIN':     \n",
    "                        ASIN = row.find('td',{'class':'a-size-base'}).get_text().strip()\n",
    "                        product['ASIN']= ASIN\n",
    "\n",
    "        product_list.append(product_info)\n",
    "        all_products = pd.DataFrame(product_list)\n",
    "    # Saves into a cvs file that can be accessed from the datasets folder\n",
    "    return all_products.to_csv(f\"datasets/{filename}.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
